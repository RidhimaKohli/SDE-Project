{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdespacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODfo9smQV7ZO",
        "outputId": "c22741f2-c0e3-4f05-fdcd-f1f3c17015cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1280, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Loading CSV file\n",
        "df = pd.read_csv (\"data.csv\", sep=\",\")\n",
        "df=df[['Summary','Classification']]\n",
        "df=df.dropna()\n",
        "df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df['Summary'])):\n",
        "  if type(df['Summary'][4])==None:\n",
        "    print(\"im here\")"
      ],
      "metadata": {
        "id": "tNh6uArPcq9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy && python -m spacy download en \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAx1xYROYn_e",
        "outputId": "34028491-e099-4819-d089-c70eacc43457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string,spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "metadata": {
        "id": "6iS-w6D0WSCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    if text==None:\n",
        "      return \"\"\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return str(text).lower()"
      ],
      "metadata": {
        "id": "T7cKpymfWYKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n"
      ],
      "metadata": {
        "id": "ze3EAJ2LWZVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n"
      ],
      "metadata": {
        "id": "uLyjvcgIWl_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_amazon['Summary'] # the features we want to analyze\n",
        "ylabels = df_amazon['Classification'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ],
      "metadata": {
        "id": "wpeDahanWueY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEEBBOY5b4Ih",
        "outputId": "1b377a8e-4a11-4180-b582-099787c4e843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "360    Upper case conversion of special characters ca...\n",
            "317    [app.class.php] Patch to add Marketplace Datab...\n",
            "263    trace-malloc broken on fxdbug-win32-tbox (due ...\n",
            "333    review: Move the quip list into the database; ...\n",
            "289    Shared library suffix incorrectly assumed to b...\n",
            "                             ...                        \n",
            "568                    Extending an abstract class fails\n",
            "548             DBCS: shown up GnomeVFS-WARNING messages\n",
            "797    DE:PR1 investorworld spacing after \"Anmelden\" ...\n",
            "196    Investigate Dependency graph builder as resour...\n",
            "518    Generate some of Roadmap from project informat...\n",
            "Name: Summary, Length: 896, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "eGCLTrgxWvYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1b1017-b922-4581-8fb2-fcbda5f9507e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f86ce759ed0>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f86dde70ef0>)),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MawxbjrVmiH_",
        "outputId": "bf0e71a0-4618-497c-8be4-81ea92ad113f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier2 = SVC(kernel='linear')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe1 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier2)])\n",
        "\n",
        "# model generation\n",
        "pipe1.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Y1f9tKllPk",
        "outputId": "9d0778e3-d886-4625-d5fc-a55be65ccc3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f86ddceb850>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f86dde70ef0>)),\n",
              "                ('classifier', SVC(kernel='linear'))])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe1.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "metadata": {
        "id": "nlfGHsGXW4fF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c1de19-947f-4e63-db16-0f822c3691b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.5260416666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "\n",
        "classifier3 = rf()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe2 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier3)])\n",
        "\n",
        "# model generation\n",
        "pipe2.fit(X_train,y_train)\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe2.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Random forest Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sezMSEaFnHki",
        "outputId": "b9c62544-1524-416d-8024-69ec43306090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest Accuracy: 0.53125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier3 = MultinomialNB()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe2 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier3)])\n",
        "\n",
        "# model generation\n",
        "pipe2.fit(X_train,y_train)\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe2.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Naive Bayes Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "metadata": {
        "id": "9eP8Nc_GnyyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7a5dc5-645b-4edd-c53e-2e0e53368ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.5260416666666666\n"
          ]
        }
      ]
    }
  ]
}