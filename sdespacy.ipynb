{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODfo9smQV7ZO",
        "outputId": "c22741f2-c0e3-4f05-fdcd-f1f3c17015cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1280, 2)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Loading CSV file\n",
        "df_amazon = pd.read_csv (\"data.csv\", sep=\",\")\n",
        "df_amazon=df_amazon[['Summary','Classification']]\n",
        "df_amazon=df_amazon.dropna()\n",
        "df_amazon.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNh6uArPcq9u"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df_amazon['Summary'])):\n",
        "  if type(df_amazon['Summary'][4])==None:\n",
        "    print(\"im here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAx1xYROYn_e",
        "outputId": "34028491-e099-4819-d089-c70eacc43457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy && python -m spacy download en \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iS-w6D0WSCr"
      },
      "outputs": [],
      "source": [
        "import string,spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7cKpymfWYKk"
      },
      "outputs": [],
      "source": [
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    if text==None:\n",
        "      return \"\"\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return str(text).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze3EAJ2LWZVV"
      },
      "outputs": [],
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLyjvcgIWl_E"
      },
      "outputs": [],
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpeDahanWueY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_amazon['Summary'] # the features we want to analyze\n",
        "ylabels = df_amazon['Classification'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEEBBOY5b4Ih",
        "outputId": "1b377a8e-4a11-4180-b582-099787c4e843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "360    Upper case conversion of special characters ca...\n",
            "317    [app.class.php] Patch to add Marketplace Datab...\n",
            "263    trace-malloc broken on fxdbug-win32-tbox (due ...\n",
            "333    review: Move the quip list into the database; ...\n",
            "289    Shared library suffix incorrectly assumed to b...\n",
            "                             ...                        \n",
            "568                    Extending an abstract class fails\n",
            "548             DBCS: shown up GnomeVFS-WARNING messages\n",
            "797    DE:PR1 investorworld spacing after \"Anmelden\" ...\n",
            "196    Investigate Dependency graph builder as resour...\n",
            "518    Generate some of Roadmap from project informat...\n",
            "Name: Summary, Length: 896, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCLTrgxWvYB",
        "outputId": "1f1b1017-b922-4581-8fb2-fcbda5f9507e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f86ce759ed0>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f86dde70ef0>)),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MawxbjrVmiH_",
        "outputId": "bf0e71a0-4618-497c-8be4-81ea92ad113f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.515625\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Y1f9tKllPk",
        "outputId": "9d0778e3-d886-4625-d5fc-a55be65ccc3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('cleaner', <__main__.predictors object at 0x7f86ddceb850>),\n",
              "                ('vectorizer',\n",
              "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x7f86dde70ef0>)),\n",
              "                ('classifier', SVC(kernel='linear'))])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier2 = SVC(kernel='linear')\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe1 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier2)])\n",
        "\n",
        "# model generation\n",
        "pipe1.fit(X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlfGHsGXW4fF",
        "outputId": "04c1de19-947f-4e63-db16-0f822c3691b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Accuracy: 0.5260416666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe1.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sezMSEaFnHki",
        "outputId": "b9c62544-1524-416d-8024-69ec43306090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest Accuracy: 0.53125\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "\n",
        "classifier3 = rf()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe2 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier3)])\n",
        "\n",
        "# model generation\n",
        "pipe2.fit(X_train,y_train)\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe2.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Random forest Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eP8Nc_GnyyD",
        "outputId": "fa7a5dc5-645b-4edd-c53e-2e0e53368ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy: 0.5260416666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier3 = MultinomialNB()\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe2 = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier3)])\n",
        "\n",
        "# model generation\n",
        "pipe2.fit(X_train,y_train)\n",
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe2.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"Naive Bayes Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjlD6Ui-treZ",
        "outputId": "a3661770-036e-4d2f-9b58-c97594d813ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "795     JA: The orange gif in center of side bar panel...\n",
            "1083                           read of unitialized memory\n",
            "887     PHP Preferences - in servers page - add a \"mak...\n",
            "1190               Security access to server and database\n",
            "762                         crash on startup with IBM JRE\n",
            "                              ...                        \n",
            "255     browser doesn't handle simple response of HTTP...\n",
            "6                           No support for Apache 2.2 yet\n",
            "487               Can't compile with bundled apr/apr-util\n",
            "291      JA:PR2 Bargain America link incorrect in sidebar\n",
            "847                        border width in tables adds up\n",
            "Name: Summary, Length: 384, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_ZiqAnTRCT2",
        "outputId": "666eecf2-8f93-4de0-ac86-34b4c596c9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "documents = []\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nksoDj7Qg7h"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCuJXChQRhmn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqWkds1aRx7u",
        "outputId": "21fad350-8421-40ee-d759-fc1c3c4f50d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,ylabels , test_size=0.2, random_state=0)\n",
        "classifier.fit(X_train, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd86XZtUSHiK",
        "outputId": "481d23fa-6f65-480d-fd5f-165ce97b9e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 4  1  0 14  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 2  4  0 26  0  0  0  0  0  0  0  0  0  1  0]\n",
            " [ 1  0  2  3  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 3  6  0 85  0  6  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  0  0 23  0 18  0  0  0  0  0  0  0  1  0]\n",
            " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  1  0  6  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 0  1  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  6  0  0  0  0  0  1  0  0  0  0  0]\n",
            " [ 0  0  0  3  0  1  0  0  0  0  1  0  0  0  0]\n",
            " [ 0  0  0  3  0  0  0  0  0  0  0  7  0  0  0]\n",
            " [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  2  0  2  0  0  0  0  0  0  0 10  0]\n",
            " [ 0  1  0  1  0  0  0  0  0  0  0  0  0  0  1]]\n",
            "                              precision    recall  f1-score   support\n",
            "\n",
            "                   Add issue       0.33      0.20      0.25        20\n",
            "         Configuration issue       0.27      0.12      0.17        33\n",
            "      Database-related issue       1.00      0.33      0.50         6\n",
            "            Functional issue       0.49      0.85      0.62       100\n",
            "           Functional issue        0.00      0.00      0.00         1\n",
            "           GUI-related issue       0.62      0.41      0.49        44\n",
            "          GUI-related issue        0.00      0.00      0.00         2\n",
            "               Network issue       1.00      0.12      0.22         8\n",
            "              Network issue        0.00      0.00      0.00         2\n",
            "           Performance issue       1.00      0.14      0.25         7\n",
            "Permission/Deprecation issue       1.00      0.20      0.33         5\n",
            "              Security issue       1.00      0.70      0.82        10\n",
            "             Security issue        0.00      0.00      0.00         1\n",
            "     Test Code-related issue       0.77      0.71      0.74        14\n",
            "          info release issue       1.00      0.33      0.50         3\n",
            "\n",
            "                    accuracy                           0.52       256\n",
            "                   macro avg       0.57      0.28      0.33       256\n",
            "                weighted avg       0.55      0.52      0.48       256\n",
            "\n",
            "0.5234375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of sdespacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
